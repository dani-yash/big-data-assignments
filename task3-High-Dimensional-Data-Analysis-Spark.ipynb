{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "# EECS 4415 - Task 3\n",
        "## High-Dimensional Data with Spark K-Means & PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's set up Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "outputId": "fe75d95c-e64a-4436-e31f-0fd95f2ecc79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "KH91tEik9zZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM"
      },
      "source": [
        "In this Colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2"
      },
      "source": [
        "For convenience, we first\n",
        "\n",
        "*   construct a Pandas dataframe\n",
        "*   tune the schema\n",
        "*   and convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w",
        "outputId": "6a4da5cd-1ee2-401a-99bd-5089b094f424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtR1xRvonxiO"
      },
      "source": [
        "With the next cell, we build the two data structures that we will be using throughout this Colab:\n",
        "\n",
        "\n",
        "*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP23Xkgwi0SD"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the Setup and Data Preprocessing stages, you are now ready to cluster the data with the [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) algorithm included in MLlib (Spark's Machine Learning library).\n",
        "Set the ```k``` parameter to **2** and seed to **1**, fit the model, and the compute and print the [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) (i.e., a measure of quality of the obtained clustering, here we use squared euclidean distance). Note: K-means in Spark’s MLlib library is distributed by default.\n",
        "\n",
        "**IMPORTANT:** use the MLlib implementation of the Silhouette score (via ```ClusteringEvaluator```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVIfPHZwWaE",
        "outputId": "80c1238a-f94a-4106-a6de-ff06dcc3745c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' 8-9 lines of code in total expected but can differ based on your style.\n",
        "The running time should be less than 1 minute'''\n",
        "# YOUR CODE HERE\n",
        "\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Initialize KMeans with k=2 and seed=1\n",
        "kmeans = KMeans(k=2, seed=1)\n",
        "\n",
        "# Fit the model on the features\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(features)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator(metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette with squared euclidean distance = {silhouette}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GB09n7sqTO6"
      },
      "source": [
        "Take the predictions produced by K-means, and compare them with the ```labels``` variable (i.e., the ground truth from our dataset).  \n",
        "\n",
        "Compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other), please use the best case scenario since the output cluster ids can be a permutation of labels.\n",
        "\n",
        "*HINT*: you can use ```np.count_nonzero(series_a == series_b)``` to quickly compute the element-wise comparison of two series.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhC3APIPPM5",
        "outputId": "2af08018-0f23-4f56-9e51-70cdd448de29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' 4 lines of code in total expected but can differ based on your style.'''\n",
        "# YOUR CODE HERE\n",
        "\n",
        "import numpy as np\n",
        "import builtins\n",
        "\n",
        "# Convert predictions to Pandas Series\n",
        "preds = predictions.select('prediction').toPandas()['prediction']\n",
        "\n",
        "# Since cluster IDs may not match labels, check both possibilities\n",
        "correct_labels = np.count_nonzero(preds == labels)\n",
        "correct_labels_inverted = np.count_nonzero(preds == 1 - labels)\n",
        "\n",
        "# Take the maximum of the two\n",
        "correct = builtins.max(correct_labels, correct_labels_inverted)\n",
        "print(f\"Number of data points clustered correctly: {correct}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points clustered correctly: 486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLIprM1JsdTU"
      },
      "source": [
        "Now perform dimensionality reduction on the ```features``` using the [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) statistical procedure, available as well in MLlib.\n",
        "\n",
        "Set the ```k``` parameter to **2**, effectively reducing the dataset size of a **15X** factor. Show top 20 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J8JMDkSb24",
        "outputId": "d5cf39e6-a7ef-4932-8b40-81acb4d9a989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' 6 lines of code in total expected but can differ based on your style.\n",
        "The running time should be less than 30 seconds.'''\n",
        "# YOUR CODE HERE\n",
        "\n",
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "# Initialize PCA with k=2\n",
        "pca = PCA(k=2, inputCol='features', outputCol='pcaFeatures')\n",
        "\n",
        "# Fit the PCA model on the features\n",
        "pca_model = pca.fit(features)\n",
        "\n",
        "# Transform the features using the PCA model\n",
        "pca_result = pca_model.transform(features).select('pcaFeatures')\n",
        "\n",
        "# Show the top 20 rows\n",
        "pca_result.show(20, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------+\n",
            "|pcaFeatures                              |\n",
            "+-----------------------------------------+\n",
            "|[-2260.013886292542,-187.96030122263687] |\n",
            "|[-2368.9937557820544,121.58742425815537] |\n",
            "|[-2095.6652015478608,145.11398565870124] |\n",
            "|[-692.6905100570509,38.57692259208171]   |\n",
            "|[-2030.2124927427067,295.2979839927931]  |\n",
            "|[-888.2800535760762,26.079796157025662]  |\n",
            "|[-1921.0822124748454,58.807572473099455] |\n",
            "|[-1074.7813350047968,31.771227808469558] |\n",
            "|[-908.5784781618834,63.83075279044635]   |\n",
            "|[-861.5784494075684,40.57073549705316]   |\n",
            "|[-1404.5591306499475,88.23218257736251]  |\n",
            "|[-1524.2324408687823,-3.2630573167779313]|\n",
            "|[-1734.385647746416,273.16267815114594]  |\n",
            "|[-1162.914003223036,217.6348180834464]   |\n",
            "|[-903.4301030498837,135.6151766608479]   |\n",
            "|[-1155.8759954206853,76.8088938374218]   |\n",
            "|[-1335.7294321308073,-2.4684005450354585]|\n",
            "|[-1547.2640922523092,3.805675972574516]  |\n",
            "|[-2714.964765181216,-164.37610864258835] |\n",
            "|[-908.2502671870881,118.21642008223107]  |\n",
            "+-----------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8leQR4-atMAl"
      },
      "source": [
        "Now run K-means with the same parameters as above, but on the ```pcaFeatures``` produced by the PCA reduction you just executed.\n",
        "\n",
        "Compute and print the Silhouette score, as well as the number of data points that have been clustered correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_snSSj5k2y5",
        "outputId": "7752f85e-2710-4428-8320-6cd814623877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "''' 8-11 lines of code in total expected but can differ based on your style.'''\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Initialize KMeans on PCA features\n",
        "kmeans_pca = KMeans(k=2, seed=1, featuresCol='pcaFeatures')\n",
        "\n",
        "# Fit the model on the PCA-reduced features\n",
        "model_pca = kmeans_pca.fit(pca_result)\n",
        "\n",
        "# Make predictions\n",
        "predictions_pca = model_pca.transform(pca_result)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator_pca = ClusteringEvaluator(featuresCol='pcaFeatures',\n",
        "                                    metricName='silhouette',\n",
        "                                    distanceMeasure='squaredEuclidean')\n",
        "\n",
        "silhouette_pca = evaluator_pca.evaluate(predictions_pca)\n",
        "print(f\"Silhouette with squared euclidean distance on PCA features = {silhouette_pca}\")\n",
        "\n",
        "# Get predictions into a Pandas Series\n",
        "preds_pca = predictions_pca.select('prediction').toPandas()['prediction']\n",
        "\n",
        "# Compare with original labels\n",
        "correct_labels_pca = np.count_nonzero(preds_pca == labels)\n",
        "correct_labels_pca_inverted = np.count_nonzero(preds_pca == 1 - labels)\n",
        "\n",
        "# Take the maximum of the two\n",
        "correct_pca = builtins.max(correct_labels_pca, correct_labels_pca_inverted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance on PCA features = 0.8348610363444832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report the number of data points that have been clustered correctly."
      ],
      "metadata": {
        "id": "JpRVp5c4azXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 1 line of code in total expected but can differ based on your style.'''\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(f\"Number of data points clustered correctly after PCA: {correct_pca}\")"
      ],
      "metadata": {
        "id": "EQsH2YtabmDM",
        "outputId": "f48ad8e5-0f1e-4458-d033-5d896545ec50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points clustered correctly after PCA: 486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment based on these scores on K-Means without and with PCA (2 sentences)"
      ],
      "metadata": {
        "id": "b2J_evesbBzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite reducing the dataset from 30 features to just 2 principal components, the clustering performance remained virtually the same, as evidenced by similar Silhouette scores and the same number of correctly clustered data points. This indicates that the first two principal components effectively captured the essential structure of the data necessary for accurate clustering."
      ],
      "metadata": {
        "id": "g85fGTW6cLEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a paragraph of conclusions below summarizing your insights."
      ],
      "metadata": {
        "id": "J4T7ivmrAZ2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we observed that applying PCA to reduce the dimensionality of the Breast Cancer Wisconsin dataset did not negatively impact the clustering results. The first two principal components preserved the majority of the variance and the intrinsic cluster structure of the data. Consequently, K-Means clustering performed equally well on both the original and the reduced datasets. This demonstrates that PCA can be an effective tool for dimensionality reduction without sacrificing clustering performance, especially when the primary components capture a significant portion of the data’s variance. It highlights the importance of analyzing the explained variance when applying PCA to ensure that the reduced dataset remains representative of the original data."
      ],
      "metadata": {
        "id": "onTR-coRAdlB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrXJyVNP2AI"
      },
      "source": [
        "Once you obtained the desired results, **head over to eClass and submit your solution for this Colab**!"
      ]
    }
  ]
}